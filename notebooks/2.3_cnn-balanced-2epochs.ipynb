{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f7fc152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# https://blog.paperspace.com/writing-cnns-from-scratch-in-pytorch/\n",
    "import os\n",
    "import ast\n",
    "from PIL import Image\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import helper\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "# import torch.utils.data as data\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torchsummary import summary\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "num_classes = 12\n",
    "learning_rate = 0.001\n",
    "num_epochs = 2\n",
    "batch_size = 64\n",
    "patience = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "ROOT_DIR = \"/workspace/resnet/\"\n",
    "ROOT_DATA_DIR = \"/workspace/resnet/data/raw\"\n",
    "ARCHIVE_DIR = os.path.join(ROOT_DATA_DIR, \"archive.zip\")\n",
    "DATA_DIR = os.path.join(ROOT_DATA_DIR)#, \"labelme-12-50k\")\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "TRAIN_ANNOT_PATH = os.path.join(TRAIN_DIR, \"annotation.txt\")\n",
    "TEST_DIR = os.path.join(DATA_DIR, \"test\")\n",
    "TEST_ANNOT_PATH = os.path.join(TEST_DIR, \"annotation.txt\")\n",
    "CLASSES_TXT_PATH = os.path.join(DATA_DIR, \"classes.txt\")\n",
    "INTERIM_DATA_DIR = \"/workspace/resnet/data/interim/\"\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"models\")\n",
    "CNN_PATH = os.path.join(MODEL_DIR, \"2.3_cnn-balanced-2epochs\")\n",
    "\n",
    "# import zipfile\n",
    "# with zipfile.ZipFile(ARCHIVE_DIR, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(ROOT_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a023b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83419f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_balanced = pd.read_csv(os.path.join(INTERIM_DATA_DIR,\"balanced-train-40000.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(INTERIM_DATA_DIR,\"test-10000.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08e6e974",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "            [transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,0.5,0.5),(0.5, 0.5, 0.5))]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3617ed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self,annot_df,transform = None):\n",
    "        self.annotations = annot_df\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        img = Image.open(self.annotations.img_path[index])\n",
    "        y_label = torch.tensor(int(self.annotations.int_label[index]))\n",
    "        if self.transform:\n",
    "            image = transform(img)\n",
    "        else:\n",
    "            image = img\n",
    "        return (image,y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3233bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f92eca075e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_data = MyDataset(train_df,transform = transform)\n",
    "test_data = MyDataset(test_df,transform = transform)\n",
    "train_data_balanced = MyDataset(train_df_balanced,transform = transform)\n",
    "\n",
    "\n",
    "# train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True)\n",
    "train_loader_balanced = DataLoader(dataset = train_data_balanced, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89f66bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     3438\n",
       "9     3422\n",
       "7     3392\n",
       "6     3325\n",
       "11    3317\n",
       "10    3312\n",
       "8     3305\n",
       "1     3303\n",
       "2     3302\n",
       "4     3297\n",
       "5     3295\n",
       "3     3292\n",
       "Name: int_label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_balanced.int_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78a8bc1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv_l1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv_l2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv_l3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv_l4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv_l5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv_l6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=50176, out_features=128, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=128, out_features=12, bias=True)\n",
      ")\n",
      "Epoch [1/2], Loss: 2.4812\n",
      "Epoch [1/2], Loss: 2.4890\n",
      "Epoch [1/2], Loss: 2.4837\n",
      "Epoch [1/2], Loss: 2.4863\n",
      "Epoch [1/2], Loss: 2.4872\n",
      "Epoch [1/2], Loss: 2.4955\n",
      "Epoch [1/2], Loss: 2.4837\n",
      "Epoch [1/2], Loss: 2.4931\n",
      "Epoch [1/2], Loss: 2.4849\n",
      "Epoch [1/2], Loss: 2.4859\n",
      "Epoch [1/2], Loss: 2.4779\n",
      "Epoch [1/2], Loss: 2.4930\n",
      "Epoch [1/2], Loss: 2.4873\n",
      "Epoch [1/2], Loss: 2.4766\n",
      "Epoch [1/2], Loss: 2.4910\n",
      "Epoch [1/2], Loss: 2.4892\n",
      "Epoch [1/2], Loss: 2.4790\n",
      "Epoch [1/2], Loss: 2.4773\n",
      "Epoch [1/2], Loss: 2.4918\n",
      "Epoch [1/2], Loss: 2.4874\n",
      "Epoch [1/2], Loss: 2.4884\n",
      "Epoch [1/2], Loss: 2.4721\n",
      "Epoch [1/2], Loss: 2.4859\n",
      "Epoch [1/2], Loss: 2.4931\n",
      "Epoch [1/2], Loss: 2.4875\n",
      "Epoch [1/2], Loss: 2.4835\n",
      "Epoch [1/2], Loss: 2.4838\n",
      "Epoch [1/2], Loss: 2.4967\n",
      "Epoch [1/2], Loss: 2.4778\n",
      "Epoch [1/2], Loss: 2.4929\n",
      "Epoch [1/2], Loss: 2.4808\n",
      "Epoch [1/2], Loss: 2.4791\n",
      "Epoch [1/2], Loss: 2.4884\n",
      "Epoch [1/2], Loss: 2.4749\n",
      "Epoch [1/2], Loss: 2.4822\n",
      "Epoch [1/2], Loss: 2.4893\n",
      "Epoch [1/2], Loss: 2.4827\n",
      "Epoch [1/2], Loss: 2.4853\n",
      "Epoch [1/2], Loss: 2.4847\n",
      "Epoch [1/2], Loss: 2.4917\n",
      "Epoch [1/2], Loss: 2.4818\n",
      "Epoch [1/2], Loss: 2.4760\n",
      "Epoch [1/2], Loss: 2.4813\n",
      "Epoch [1/2], Loss: 2.4839\n",
      "Epoch [1/2], Loss: 2.4868\n",
      "Epoch [1/2], Loss: 2.4907\n",
      "Epoch [1/2], Loss: 2.4793\n",
      "Epoch [1/2], Loss: 2.4853\n",
      "Epoch [1/2], Loss: 2.4827\n",
      "Epoch [1/2], Loss: 2.4787\n",
      "Epoch [1/2], Loss: 2.4818\n",
      "Epoch [1/2], Loss: 2.4782\n",
      "Epoch [1/2], Loss: 2.4814\n",
      "Epoch [1/2], Loss: 2.4866\n",
      "Epoch [1/2], Loss: 2.4871\n",
      "Epoch [1/2], Loss: 2.4877\n",
      "Epoch [1/2], Loss: 2.4859\n",
      "Epoch [1/2], Loss: 2.4795\n",
      "Epoch [1/2], Loss: 2.4848\n",
      "Epoch [1/2], Loss: 2.4785\n",
      "Epoch [1/2], Loss: 2.4823\n",
      "Epoch [1/2], Loss: 2.4878\n",
      "Epoch [1/2], Loss: 2.4770\n",
      "Epoch [1/2], Loss: 2.4800\n",
      "Epoch [1/2], Loss: 2.4855\n",
      "Epoch [1/2], Loss: 2.4809\n",
      "Epoch [1/2], Loss: 2.4819\n",
      "Epoch [1/2], Loss: 2.4846\n",
      "Epoch [1/2], Loss: 2.4795\n",
      "Epoch [1/2], Loss: 2.4734\n",
      "Epoch [1/2], Loss: 2.4806\n",
      "Epoch [1/2], Loss: 2.4825\n",
      "Epoch [1/2], Loss: 2.4839\n",
      "Epoch [1/2], Loss: 2.4765\n",
      "Epoch [1/2], Loss: 2.4782\n",
      "Epoch [1/2], Loss: 2.4859\n",
      "Epoch [1/2], Loss: 2.4782\n",
      "Epoch [1/2], Loss: 2.4808\n",
      "Epoch [1/2], Loss: 2.4720\n",
      "Epoch [1/2], Loss: 2.4770\n",
      "Epoch [1/2], Loss: 2.4847\n",
      "Epoch [1/2], Loss: 2.4751\n",
      "Epoch [1/2], Loss: 2.4778\n",
      "Epoch [1/2], Loss: 2.4702\n",
      "Epoch [1/2], Loss: 2.4775\n",
      "Epoch [1/2], Loss: 2.4749\n",
      "Epoch [1/2], Loss: 2.4842\n",
      "Epoch [1/2], Loss: 2.4844\n",
      "Epoch [1/2], Loss: 2.4787\n",
      "Epoch [1/2], Loss: 2.4724\n",
      "Epoch [1/2], Loss: 2.4692\n",
      "Epoch [1/2], Loss: 2.4775\n",
      "Epoch [1/2], Loss: 2.4743\n",
      "Epoch [1/2], Loss: 2.4758\n",
      "Epoch [1/2], Loss: 2.4733\n",
      "Epoch [1/2], Loss: 2.4765\n",
      "Epoch [1/2], Loss: 2.4738\n",
      "Epoch [1/2], Loss: 2.4782\n",
      "Epoch [1/2], Loss: 2.4742\n",
      "Epoch [1/2], Loss: 2.4819\n",
      "Epoch [1/2], Loss: 2.4684\n",
      "Epoch [1/2], Loss: 2.4733\n",
      "Epoch [1/2], Loss: 2.4676\n",
      "Epoch [1/2], Loss: 2.4782\n",
      "Epoch [1/2], Loss: 2.4754\n",
      "Epoch [1/2], Loss: 2.4732\n",
      "Epoch [1/2], Loss: 2.4734\n",
      "Epoch [1/2], Loss: 2.4666\n",
      "Epoch [1/2], Loss: 2.4681\n",
      "Epoch [1/2], Loss: 2.4687\n",
      "Epoch [1/2], Loss: 2.4823\n",
      "Epoch [1/2], Loss: 2.4769\n",
      "Epoch [1/2], Loss: 2.4815\n",
      "Epoch [1/2], Loss: 2.4715\n",
      "Epoch [1/2], Loss: 2.4776\n",
      "Epoch [1/2], Loss: 2.4633\n",
      "Epoch [1/2], Loss: 2.4680\n",
      "Epoch [1/2], Loss: 2.4717\n",
      "Epoch [1/2], Loss: 2.4671\n",
      "Epoch [1/2], Loss: 2.4742\n",
      "Epoch [1/2], Loss: 2.4692\n",
      "Epoch [1/2], Loss: 2.4648\n",
      "Epoch [1/2], Loss: 2.4784\n",
      "Epoch [1/2], Loss: 2.4689\n",
      "Epoch [1/2], Loss: 2.4608\n",
      "Epoch [1/2], Loss: 2.4654\n",
      "Epoch [1/2], Loss: 2.4569\n",
      "Epoch [1/2], Loss: 2.4651\n",
      "Epoch [1/2], Loss: 2.4662\n",
      "Epoch [1/2], Loss: 2.4578\n",
      "Epoch [1/2], Loss: 2.4477\n",
      "Epoch [1/2], Loss: 2.4611\n",
      "Epoch [1/2], Loss: 2.4615\n",
      "Epoch [1/2], Loss: 2.4789\n",
      "Epoch [1/2], Loss: 2.4725\n",
      "Epoch [1/2], Loss: 2.4550\n",
      "Epoch [1/2], Loss: 2.4637\n",
      "Epoch [1/2], Loss: 2.4567\n",
      "Epoch [1/2], Loss: 2.4565\n",
      "Epoch [1/2], Loss: 2.4585\n",
      "Epoch [1/2], Loss: 2.4716\n",
      "Epoch [1/2], Loss: 2.4813\n",
      "Epoch [1/2], Loss: 2.4463\n",
      "Epoch [1/2], Loss: 2.4567\n",
      "Epoch [1/2], Loss: 2.4501\n",
      "Epoch [1/2], Loss: 2.4655\n",
      "Epoch [1/2], Loss: 2.4591\n",
      "Epoch [1/2], Loss: 2.4559\n",
      "Epoch [1/2], Loss: 2.4484\n",
      "Epoch [1/2], Loss: 2.4368\n",
      "Epoch [1/2], Loss: 2.4556\n",
      "Epoch [1/2], Loss: 2.4389\n",
      "Epoch [1/2], Loss: 2.4560\n",
      "Epoch [1/2], Loss: 2.4155\n",
      "Epoch [1/2], Loss: 2.4430\n",
      "Epoch [1/2], Loss: 2.4299\n",
      "Epoch [1/2], Loss: 2.4261\n",
      "Epoch [1/2], Loss: 2.4350\n",
      "Epoch [1/2], Loss: 2.4247\n",
      "Epoch [1/2], Loss: 2.4596\n",
      "Epoch [1/2], Loss: 2.4415\n",
      "Epoch [1/2], Loss: 2.4178\n",
      "Epoch [1/2], Loss: 2.4397\n",
      "Epoch [1/2], Loss: 2.4350\n",
      "Epoch [1/2], Loss: 2.3918\n",
      "Epoch [1/2], Loss: 2.4003\n",
      "Epoch [1/2], Loss: 2.4315\n",
      "Epoch [1/2], Loss: 2.4049\n",
      "Epoch [1/2], Loss: 2.3937\n",
      "Epoch [1/2], Loss: 2.4059\n",
      "Epoch [1/2], Loss: 2.4033\n",
      "Epoch [1/2], Loss: 2.4211\n",
      "Epoch [1/2], Loss: 2.3922\n",
      "Epoch [1/2], Loss: 2.4109\n",
      "Epoch [1/2], Loss: 2.3775\n",
      "Epoch [1/2], Loss: 2.4573\n",
      "Epoch [1/2], Loss: 2.3917\n",
      "Epoch [1/2], Loss: 2.3865\n",
      "Epoch [1/2], Loss: 2.4215\n",
      "Epoch [1/2], Loss: 2.4293\n",
      "Epoch [1/2], Loss: 2.4034\n",
      "Epoch [1/2], Loss: 2.3562\n",
      "Epoch [1/2], Loss: 2.3939\n",
      "Epoch [1/2], Loss: 2.3671\n",
      "Epoch [1/2], Loss: 2.3874\n",
      "Epoch [1/2], Loss: 2.3490\n",
      "Epoch [1/2], Loss: 2.3338\n",
      "Epoch [1/2], Loss: 2.3337\n",
      "Epoch [1/2], Loss: 2.3567\n",
      "Epoch [1/2], Loss: 2.2988\n",
      "Epoch [1/2], Loss: 2.3017\n",
      "Epoch [1/2], Loss: 2.2404\n",
      "Epoch [1/2], Loss: 2.2896\n",
      "Epoch [1/2], Loss: 2.3366\n",
      "Epoch [1/2], Loss: 2.3355\n",
      "Epoch [1/2], Loss: 2.3321\n",
      "Epoch [1/2], Loss: 2.1671\n",
      "Epoch [1/2], Loss: 2.2087\n",
      "Epoch [1/2], Loss: 2.2093\n",
      "Epoch [1/2], Loss: 2.2912\n",
      "Epoch [1/2], Loss: 2.1889\n",
      "Epoch [1/2], Loss: 2.2651\n",
      "Epoch [1/2], Loss: 2.1547\n",
      "Epoch [1/2], Loss: 2.3161\n",
      "Epoch [1/2], Loss: 2.3802\n",
      "Epoch [1/2], Loss: 2.3198\n",
      "Epoch [1/2], Loss: 2.3229\n",
      "Epoch [1/2], Loss: 2.2786\n",
      "Epoch [1/2], Loss: 2.1924\n",
      "Epoch [1/2], Loss: 2.2996\n",
      "Epoch [1/2], Loss: 2.3858\n",
      "Epoch [1/2], Loss: 2.1611\n",
      "Epoch [1/2], Loss: 2.1548\n",
      "Epoch [1/2], Loss: 2.2000\n",
      "Epoch [1/2], Loss: 2.1511\n",
      "Epoch [1/2], Loss: 2.0444\n",
      "Epoch [1/2], Loss: 2.0303\n",
      "Epoch [1/2], Loss: 2.2679\n",
      "Epoch [1/2], Loss: 2.0433\n",
      "Epoch [1/2], Loss: 2.0814\n",
      "Epoch [1/2], Loss: 2.3112\n",
      "Epoch [1/2], Loss: 2.2202\n",
      "Epoch [1/2], Loss: 2.3003\n",
      "Epoch [1/2], Loss: 2.3093\n",
      "Epoch [1/2], Loss: 2.2880\n",
      "Epoch [1/2], Loss: 2.2233\n",
      "Epoch [1/2], Loss: 2.0148\n",
      "Epoch [1/2], Loss: 2.2022\n",
      "Epoch [1/2], Loss: 2.0581\n",
      "Epoch [1/2], Loss: 2.3198\n",
      "Epoch [1/2], Loss: 2.0838\n",
      "Epoch [1/2], Loss: 2.1379\n",
      "Epoch [1/2], Loss: 2.0609\n",
      "Epoch [1/2], Loss: 2.0796\n",
      "Epoch [1/2], Loss: 2.0879\n",
      "Epoch [1/2], Loss: 2.1127\n",
      "Epoch [1/2], Loss: 2.1496\n",
      "Epoch [1/2], Loss: 2.1424\n",
      "Epoch [1/2], Loss: 2.1523\n",
      "Epoch [1/2], Loss: 1.9087\n",
      "Epoch [1/2], Loss: 2.2891\n",
      "Epoch [1/2], Loss: 1.9412\n",
      "Epoch [1/2], Loss: 2.0329\n",
      "Epoch [1/2], Loss: 2.1426\n",
      "Epoch [1/2], Loss: 2.3615\n",
      "Epoch [1/2], Loss: 2.1020\n",
      "Epoch [1/2], Loss: 2.0688\n",
      "Epoch [1/2], Loss: 2.0541\n",
      "Epoch [1/2], Loss: 2.2368\n",
      "Epoch [1/2], Loss: 2.2133\n",
      "Epoch [1/2], Loss: 2.0875\n",
      "Epoch [1/2], Loss: 1.9195\n",
      "Epoch [1/2], Loss: 2.0208\n",
      "Epoch [1/2], Loss: 2.3084\n",
      "Epoch [1/2], Loss: 1.8829\n",
      "Epoch [1/2], Loss: 2.1782\n",
      "Epoch [1/2], Loss: 1.9236\n",
      "Epoch [1/2], Loss: 2.0338\n",
      "Epoch [1/2], Loss: 1.9907\n",
      "Epoch [1/2], Loss: 1.9354\n",
      "Epoch [1/2], Loss: 2.0002\n",
      "Epoch [1/2], Loss: 2.1148\n",
      "Epoch [1/2], Loss: 1.9454\n",
      "Epoch [1/2], Loss: 1.9897\n",
      "Epoch [1/2], Loss: 1.9406\n",
      "Epoch [1/2], Loss: 2.1062\n",
      "Epoch [1/2], Loss: 2.1840\n",
      "Epoch [1/2], Loss: 2.0744\n",
      "Epoch [1/2], Loss: 1.7956\n",
      "Epoch [1/2], Loss: 1.9698\n",
      "Epoch [1/2], Loss: 2.2189\n",
      "Epoch [1/2], Loss: 1.9862\n",
      "Epoch [1/2], Loss: 2.2854\n",
      "Epoch [1/2], Loss: 2.0824\n",
      "Epoch [1/2], Loss: 2.1079\n",
      "Epoch [1/2], Loss: 2.0414\n",
      "Epoch [1/2], Loss: 1.8284\n",
      "Epoch [1/2], Loss: 1.9968\n",
      "Epoch [1/2], Loss: 1.8213\n",
      "Epoch [1/2], Loss: 2.1583\n",
      "Epoch [1/2], Loss: 1.9471\n",
      "Epoch [1/2], Loss: 1.7624\n",
      "Epoch [1/2], Loss: 2.0921\n",
      "Epoch [1/2], Loss: 2.0568\n",
      "Epoch [1/2], Loss: 2.1015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Loss: 2.0125\n",
      "Epoch [1/2], Loss: 2.0534\n",
      "Epoch [1/2], Loss: 1.8138\n",
      "Epoch [1/2], Loss: 2.1418\n",
      "Epoch [1/2], Loss: 2.0031\n",
      "Epoch [1/2], Loss: 2.1343\n",
      "Epoch [1/2], Loss: 2.2274\n",
      "Epoch [1/2], Loss: 1.9956\n",
      "Epoch [1/2], Loss: 1.9587\n",
      "Epoch [1/2], Loss: 1.9313\n",
      "Epoch [1/2], Loss: 1.8295\n",
      "Epoch [1/2], Loss: 2.1304\n",
      "Epoch [1/2], Loss: 2.0833\n",
      "Epoch [1/2], Loss: 2.1069\n",
      "Epoch [1/2], Loss: 2.1773\n",
      "Epoch [1/2], Loss: 1.7665\n",
      "Epoch [1/2], Loss: 1.7528\n",
      "Epoch [1/2], Loss: 2.0668\n",
      "Epoch [1/2], Loss: 2.1677\n",
      "Epoch [1/2], Loss: 1.9939\n",
      "Epoch [1/2], Loss: 2.0310\n",
      "Epoch [1/2], Loss: 1.8210\n",
      "Epoch [1/2], Loss: 1.9768\n",
      "Epoch [1/2], Loss: 1.7698\n",
      "Epoch [1/2], Loss: 1.8926\n",
      "Epoch [1/2], Loss: 1.9441\n",
      "Epoch [1/2], Loss: 1.9589\n",
      "Epoch [1/2], Loss: 1.7564\n",
      "Epoch [1/2], Loss: 1.9165\n",
      "Epoch [1/2], Loss: 2.0138\n",
      "Epoch [1/2], Loss: 1.9949\n",
      "Epoch [1/2], Loss: 1.9434\n",
      "Epoch [1/2], Loss: 2.1154\n",
      "Epoch [1/2], Loss: 1.7220\n",
      "Epoch [1/2], Loss: 2.0170\n",
      "Epoch [1/2], Loss: 1.9082\n",
      "Epoch [1/2], Loss: 2.0663\n",
      "Epoch [1/2], Loss: 2.1460\n",
      "Epoch [1/2], Loss: 1.8551\n",
      "Epoch [1/2], Loss: 1.7309\n",
      "Epoch [1/2], Loss: 2.1131\n",
      "Epoch [1/2], Loss: 1.9340\n",
      "Epoch [1/2], Loss: 1.9014\n",
      "Epoch [1/2], Loss: 1.7540\n",
      "Epoch [1/2], Loss: 1.7925\n",
      "Epoch [1/2], Loss: 1.8767\n",
      "Epoch [1/2], Loss: 1.9251\n",
      "Epoch [1/2], Loss: 1.8410\n",
      "Epoch [1/2], Loss: 1.9311\n",
      "Epoch [1/2], Loss: 1.7867\n",
      "Epoch [1/2], Loss: 1.8654\n",
      "Epoch [1/2], Loss: 2.1013\n",
      "Epoch [1/2], Loss: 2.0213\n",
      "Epoch [1/2], Loss: 1.8537\n",
      "Epoch [1/2], Loss: 1.8856\n",
      "Epoch [1/2], Loss: 1.8046\n",
      "Epoch [1/2], Loss: 1.8596\n",
      "Epoch [1/2], Loss: 2.2035\n",
      "Epoch [1/2], Loss: 1.9553\n",
      "Epoch [1/2], Loss: 1.7517\n",
      "Epoch [1/2], Loss: 2.0389\n",
      "Epoch [1/2], Loss: 2.1490\n",
      "Epoch [1/2], Loss: 1.9496\n",
      "Epoch [1/2], Loss: 1.8162\n",
      "Epoch [1/2], Loss: 1.7205\n",
      "Epoch [1/2], Loss: 1.7202\n",
      "Epoch [1/2], Loss: 2.0615\n",
      "Epoch [1/2], Loss: 1.6741\n",
      "Epoch [1/2], Loss: 1.8580\n",
      "Epoch [1/2], Loss: 1.7117\n",
      "Epoch [1/2], Loss: 1.7833\n",
      "Epoch [1/2], Loss: 2.0665\n",
      "Epoch [1/2], Loss: 1.7476\n",
      "Epoch [1/2], Loss: 1.9896\n",
      "Epoch [1/2], Loss: 1.8652\n",
      "Epoch [1/2], Loss: 1.9119\n",
      "Epoch [1/2], Loss: 1.9439\n",
      "Epoch [1/2], Loss: 1.6180\n",
      "Epoch [1/2], Loss: 1.9760\n",
      "Epoch [1/2], Loss: 1.8233\n",
      "Epoch [1/2], Loss: 1.7978\n",
      "Epoch [1/2], Loss: 1.8108\n",
      "Epoch [1/2], Loss: 2.1238\n",
      "Epoch [1/2], Loss: 1.6569\n",
      "Epoch [1/2], Loss: 1.6281\n",
      "Epoch [1/2], Loss: 2.0192\n",
      "Epoch [1/2], Loss: 1.7850\n",
      "Epoch [1/2], Loss: 1.7213\n",
      "Epoch [1/2], Loss: 1.8184\n",
      "Epoch [1/2], Loss: 1.7244\n",
      "Epoch [1/2], Loss: 1.8569\n",
      "Epoch [1/2], Loss: 1.7672\n",
      "Epoch [1/2], Loss: 1.7482\n",
      "Epoch [1/2], Loss: 1.9264\n",
      "Epoch [1/2], Loss: 1.9584\n",
      "Epoch [1/2], Loss: 1.7394\n",
      "Epoch [1/2], Loss: 1.9688\n",
      "Epoch [1/2], Loss: 1.8769\n",
      "Epoch [1/2], Loss: 1.6410\n",
      "Epoch [1/2], Loss: 1.9221\n",
      "Epoch [1/2], Loss: 2.0243\n",
      "Epoch [1/2], Loss: 1.6602\n",
      "Epoch [1/2], Loss: 1.6712\n",
      "Epoch [1/2], Loss: 1.7300\n",
      "Epoch [1/2], Loss: 1.8959\n",
      "Epoch [1/2], Loss: 1.8704\n",
      "Epoch [1/2], Loss: 1.8103\n",
      "Epoch [1/2], Loss: 1.9109\n",
      "Epoch [1/2], Loss: 1.9734\n",
      "Epoch [1/2], Loss: 1.9434\n",
      "Epoch [1/2], Loss: 1.7654\n",
      "Epoch [1/2], Loss: 1.8466\n",
      "Epoch [1/2], Loss: 1.9800\n",
      "Epoch [1/2], Loss: 1.8987\n",
      "Epoch [1/2], Loss: 1.6683\n",
      "Epoch [1/2], Loss: 1.8952\n",
      "Epoch [1/2], Loss: 1.7667\n",
      "Epoch [1/2], Loss: 1.5939\n",
      "Epoch [1/2], Loss: 1.9048\n",
      "Epoch [1/2], Loss: 1.8219\n",
      "Epoch [1/2], Loss: 1.9980\n",
      "Epoch [1/2], Loss: 1.8696\n",
      "Epoch [1/2], Loss: 1.9907\n",
      "Epoch [1/2], Loss: 1.8184\n",
      "Epoch [1/2], Loss: 1.7701\n",
      "Epoch [1/2], Loss: 1.7632\n",
      "Epoch [1/2], Loss: 1.8523\n",
      "Epoch [1/2], Loss: 2.1143\n",
      "Epoch [1/2], Loss: 1.8473\n",
      "Epoch [1/2], Loss: 1.6661\n",
      "Epoch [1/2], Loss: 1.5059\n",
      "Epoch [1/2], Loss: 1.9095\n",
      "Epoch [1/2], Loss: 1.8849\n",
      "Epoch [1/2], Loss: 1.6306\n",
      "Epoch [1/2], Loss: 1.4738\n",
      "Epoch [1/2], Loss: 1.6390\n",
      "Epoch [1/2], Loss: 1.6899\n",
      "Epoch [1/2], Loss: 2.0218\n",
      "Epoch [1/2], Loss: 1.8829\n",
      "Epoch [1/2], Loss: 1.5731\n",
      "Epoch [1/2], Loss: 1.8024\n",
      "Epoch [1/2], Loss: 1.8225\n",
      "Epoch [1/2], Loss: 1.6472\n",
      "Epoch [1/2], Loss: 1.7480\n",
      "Epoch [1/2], Loss: 1.7982\n",
      "Epoch [1/2], Loss: 1.8295\n",
      "Epoch [1/2], Loss: 1.7121\n",
      "Epoch [1/2], Loss: 2.0058\n",
      "Epoch [1/2], Loss: 1.8181\n",
      "Epoch [1/2], Loss: 1.8249\n",
      "Epoch [1/2], Loss: 1.6580\n",
      "Epoch [1/2], Loss: 1.9507\n",
      "Epoch [1/2], Loss: 1.8447\n",
      "Epoch [1/2], Loss: 1.6669\n",
      "Epoch [1/2], Loss: 1.7259\n",
      "Epoch [1/2], Loss: 1.8418\n",
      "Epoch [1/2], Loss: 1.8937\n",
      "Epoch [1/2], Loss: 1.6050\n",
      "Epoch [1/2], Loss: 1.8296\n",
      "Epoch [1/2], Loss: 1.7115\n",
      "Epoch [1/2], Loss: 1.8021\n",
      "Epoch [1/2], Loss: 1.7459\n",
      "Epoch [1/2], Loss: 1.4094\n",
      "Epoch [1/2], Loss: 1.8097\n",
      "Epoch [1/2], Loss: 1.4838\n",
      "Epoch [1/2], Loss: 1.7423\n",
      "Epoch [1/2], Loss: 1.7942\n",
      "Epoch [1/2], Loss: 1.7003\n",
      "Epoch [1/2], Loss: 1.4656\n",
      "Epoch [1/2], Loss: 1.6066\n",
      "Epoch [1/2], Loss: 1.6505\n",
      "Epoch [1/2], Loss: 1.7224\n",
      "Epoch [1/2], Loss: 1.7592\n",
      "Epoch [1/2], Loss: 1.6919\n",
      "Epoch [1/2], Loss: 1.7506\n",
      "Epoch [1/2], Loss: 1.5970\n",
      "Epoch [1/2], Loss: 1.9066\n",
      "Epoch [1/2], Loss: 1.6372\n",
      "Epoch [1/2], Loss: 1.4890\n",
      "Epoch [1/2], Loss: 1.7843\n",
      "Epoch [1/2], Loss: 1.6495\n",
      "Epoch [1/2], Loss: 1.8794\n",
      "Epoch [1/2], Loss: 1.8202\n",
      "Epoch [1/2], Loss: 1.7484\n",
      "Epoch [1/2], Loss: 1.8010\n",
      "Epoch [1/2], Loss: 1.2582\n",
      "Epoch [1/2], Loss: 1.7283\n",
      "Epoch [1/2], Loss: 1.6317\n",
      "Epoch [1/2], Loss: 1.8241\n",
      "Epoch [1/2], Loss: 1.4653\n",
      "Epoch [1/2], Loss: 1.7776\n",
      "Epoch [1/2], Loss: 1.4919\n",
      "Epoch [1/2], Loss: 1.7727\n",
      "Epoch [1/2], Loss: 1.6699\n",
      "Epoch [1/2], Loss: 1.7458\n",
      "Epoch [1/2], Loss: 1.6890\n",
      "Epoch [1/2], Loss: 1.6962\n",
      "Epoch [1/2], Loss: 1.5589\n",
      "Epoch [1/2], Loss: 1.3954\n",
      "Epoch [1/2], Loss: 1.4823\n",
      "Epoch [1/2], Loss: 1.7365\n",
      "Epoch [1/2], Loss: 1.5771\n",
      "Epoch [1/2], Loss: 1.7630\n",
      "Epoch [1/2], Loss: 1.8100\n",
      "Epoch [1/2], Loss: 1.5712\n",
      "Epoch [1/2], Loss: 1.3960\n",
      "Epoch [1/2], Loss: 1.7075\n",
      "Epoch [1/2], Loss: 1.8053\n",
      "Epoch [1/2], Loss: 1.8246\n",
      "Epoch [1/2], Loss: 1.6728\n",
      "Epoch [1/2], Loss: 1.9249\n",
      "Epoch [1/2], Loss: 1.5667\n",
      "Epoch [1/2], Loss: 1.7123\n",
      "Epoch [1/2], Loss: 1.5908\n",
      "Epoch [1/2], Loss: 1.5053\n",
      "Epoch [1/2], Loss: 1.6987\n",
      "Epoch [1/2], Loss: 1.5612\n",
      "Epoch [1/2], Loss: 1.6897\n",
      "Epoch [1/2], Loss: 1.7485\n",
      "Epoch [1/2], Loss: 1.8031\n",
      "Epoch [1/2], Loss: 1.7662\n",
      "Epoch [1/2], Loss: 1.5677\n",
      "Epoch [1/2], Loss: 1.5557\n",
      "Epoch [1/2], Loss: 1.5851\n",
      "Epoch [1/2], Loss: 1.6465\n",
      "Epoch [1/2], Loss: 1.5506\n",
      "Epoch [1/2], Loss: 1.6352\n",
      "Epoch [1/2], Loss: 1.7214\n",
      "Epoch [1/2], Loss: 1.6008\n",
      "Epoch [1/2], Loss: 1.6336\n",
      "Epoch [1/2], Loss: 1.8395\n",
      "Epoch [1/2], Loss: 1.4622\n",
      "Epoch [1/2], Loss: 1.7172\n",
      "Epoch [1/2], Loss: 1.4630\n",
      "Epoch [1/2], Loss: 1.5668\n",
      "Epoch [1/2], Loss: 1.5687\n",
      "Epoch [1/2], Loss: 1.3441\n",
      "Epoch [1/2], Loss: 1.5938\n",
      "Epoch [1/2], Loss: 1.6221\n",
      "Epoch [1/2], Loss: 1.5099\n",
      "Epoch [1/2], Loss: 1.5099\n",
      "Epoch [1/2], Loss: 1.7575\n",
      "Epoch [1/2], Loss: 1.4662\n",
      "Epoch [1/2], Loss: 1.6252\n",
      "Epoch [1/2], Loss: 1.6644\n",
      "Epoch [1/2], Loss: 1.5189\n",
      "Epoch [1/2], Loss: 1.6914\n",
      "Epoch [1/2], Loss: 1.4980\n",
      "Epoch [1/2], Loss: 1.6620\n",
      "Epoch [1/2], Loss: 1.8449\n",
      "Epoch [1/2], Loss: 1.5363\n",
      "Epoch [1/2], Loss: 1.7393\n",
      "Epoch [1/2], Loss: 1.5425\n",
      "Epoch [1/2], Loss: 1.6593\n",
      "Epoch [1/2], Loss: 1.3753\n",
      "Epoch [1/2], Loss: 1.6947\n",
      "Epoch [1/2], Loss: 1.5758\n",
      "Epoch [1/2], Loss: 1.5838\n",
      "Epoch [1/2], Loss: 1.4584\n",
      "Epoch [1/2], Loss: 1.5169\n",
      "Epoch [1/2], Loss: 1.4284\n",
      "Epoch [1/2], Loss: 1.5489\n",
      "Epoch [1/2], Loss: 1.4571\n",
      "Epoch [1/2], Loss: 1.3188\n",
      "Epoch [1/2], Loss: 1.9412\n",
      "Epoch [1/2], Loss: 1.4521\n",
      "Epoch [1/2], Loss: 1.5312\n",
      "Epoch [1/2], Loss: 1.7356\n",
      "Epoch [1/2], Loss: 1.5869\n",
      "Epoch [1/2], Loss: 1.4454\n",
      "Epoch [1/2], Loss: 1.7554\n",
      "Epoch [1/2], Loss: 1.5263\n",
      "Epoch [1/2], Loss: 1.5267\n",
      "Epoch [1/2], Loss: 1.5876\n",
      "Epoch [1/2], Loss: 1.4801\n",
      "Epoch [1/2], Loss: 1.6499\n",
      "Epoch [1/2], Loss: 1.5892\n",
      "Epoch [1/2], Loss: 1.7735\n",
      "Epoch [1/2], Loss: 1.4555\n",
      "Epoch [1/2], Loss: 1.4445\n",
      "Epoch [1/2], Loss: 1.4850\n",
      "Epoch [1/2], Loss: 1.6548\n",
      "Epoch [1/2], Loss: 1.1341\n",
      "Epoch [1/2], Loss: 1.6096\n",
      "Epoch [1/2], Loss: 1.6891\n",
      "Epoch [1/2], Loss: 1.6203\n",
      "Epoch [1/2], Loss: 1.4928\n",
      "Epoch [1/2], Loss: 1.7943\n",
      "Epoch [1/2], Loss: 1.4981\n",
      "Epoch [1/2], Loss: 1.3343\n",
      "Epoch [1/2], Loss: 1.4480\n",
      "Epoch [1/2], Loss: 1.3591\n",
      "Epoch [1/2], Loss: 1.5993\n",
      "Epoch [1/2], Loss: 1.7390\n",
      "Epoch [1/2], Loss: 1.3744\n",
      "Epoch [1/2], Loss: 1.6548\n",
      "Epoch [1/2], Loss: 1.5386\n",
      "Epoch [1/2], Loss: 1.5071\n",
      "Epoch [1/2], Loss: 1.7784\n",
      "Epoch [1/2], Loss: 1.6148\n",
      "Epoch [1/2], Loss: 1.4000\n",
      "Epoch [1/2], Loss: 1.4880\n",
      "Epoch [1/2], Loss: 1.3856\n",
      "Epoch [1/2], Loss: 1.6539\n",
      "Epoch [1/2], Loss: 1.4947\n",
      "Epoch [1/2], Loss: 1.4197\n",
      "Epoch [1/2], Loss: 1.6243\n",
      "Epoch [1/2], Loss: 1.5670\n",
      "Epoch [1/2], Loss: 1.5581\n",
      "Epoch [1/2], Loss: 1.7202\n",
      "Epoch [1/2], Loss: 1.4377\n",
      "Epoch [1/2], Loss: 1.3453\n",
      "Epoch [1/2], Loss: 1.5775\n",
      "Epoch [1/2], Loss: 1.3871\n",
      "Epoch [1/2], Loss: 1.3745\n",
      "Epoch [1/2], Loss: 1.6483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Loss: 1.5406\n",
      "Epoch [1/2], Loss: 1.6887\n",
      "Epoch [1/2], Loss: 1.6429\n",
      "Epoch [1/2], Loss: 1.2843\n",
      "Epoch [1/2], Loss: 1.6536\n",
      "Epoch [1/2], Loss: 1.4942\n",
      "Epoch [1/2], Loss: 1.5599\n",
      "Epoch [1/2], Loss: 1.3161\n",
      "Epoch [1/2], Loss: 2.0185\n",
      "Epoch [1/2], Loss: 1.6520\n",
      "Epoch [1/2], Loss: 1.5060\n",
      "Epoch [1/2], Loss: 1.6989\n",
      "Epoch [1/2], Loss: 1.3928\n",
      "Epoch [1/2], Loss: 1.5167\n",
      "Epoch [1/2], Loss: 1.3822\n",
      "Epoch [1/2], Loss: 1.3636\n",
      "Epoch [1/2], Loss: 1.4093\n",
      "Epoch [1/2], Loss: 1.2877\n",
      "Epoch [1/2], Loss: 1.5283\n",
      "Epoch [1/2], Loss: 1.5593\n",
      "Epoch [1/2], Loss: 1.1961\n",
      "Epoch [1/2], Loss: 1.6584\n",
      "Epoch [1/2], Loss: 1.4067\n",
      "Epoch [1/2], Loss: 1.3103\n",
      "Epoch [1/2], Loss: 1.2550\n",
      "Epoch [1/2], Loss: 1.2550\n",
      "Epoch [2/2], Loss: 1.2650\n",
      "Epoch [2/2], Loss: 1.3770\n",
      "Epoch [2/2], Loss: 1.1319\n",
      "Epoch [2/2], Loss: 1.5264\n",
      "Epoch [2/2], Loss: 1.5841\n",
      "Epoch [2/2], Loss: 1.5545\n",
      "Epoch [2/2], Loss: 1.4779\n",
      "Epoch [2/2], Loss: 1.7865\n",
      "Epoch [2/2], Loss: 1.2644\n",
      "Epoch [2/2], Loss: 1.4868\n",
      "Epoch [2/2], Loss: 1.3027\n",
      "Epoch [2/2], Loss: 1.6099\n",
      "Epoch [2/2], Loss: 1.1206\n",
      "Epoch [2/2], Loss: 1.3867\n",
      "Epoch [2/2], Loss: 1.4846\n",
      "Epoch [2/2], Loss: 1.6521\n",
      "Epoch [2/2], Loss: 1.4343\n",
      "Epoch [2/2], Loss: 1.4390\n",
      "Epoch [2/2], Loss: 1.4591\n",
      "Epoch [2/2], Loss: 1.1109\n",
      "Epoch [2/2], Loss: 1.4828\n",
      "Epoch [2/2], Loss: 1.3005\n",
      "Epoch [2/2], Loss: 1.7391\n",
      "Epoch [2/2], Loss: 1.1604\n",
      "Epoch [2/2], Loss: 1.2974\n",
      "Epoch [2/2], Loss: 1.4931\n",
      "Epoch [2/2], Loss: 1.5967\n",
      "Epoch [2/2], Loss: 1.4978\n",
      "Epoch [2/2], Loss: 1.5559\n",
      "Epoch [2/2], Loss: 1.4772\n",
      "Epoch [2/2], Loss: 1.4315\n",
      "Epoch [2/2], Loss: 1.3870\n",
      "Epoch [2/2], Loss: 1.5538\n",
      "Epoch [2/2], Loss: 1.3583\n",
      "Epoch [2/2], Loss: 1.5008\n",
      "Epoch [2/2], Loss: 1.4360\n",
      "Epoch [2/2], Loss: 1.2075\n",
      "Epoch [2/2], Loss: 1.5664\n",
      "Epoch [2/2], Loss: 1.5303\n",
      "Epoch [2/2], Loss: 1.3344\n",
      "Epoch [2/2], Loss: 1.1538\n",
      "Epoch [2/2], Loss: 1.4669\n",
      "Epoch [2/2], Loss: 1.2408\n",
      "Epoch [2/2], Loss: 1.1180\n",
      "Epoch [2/2], Loss: 1.6229\n",
      "Epoch [2/2], Loss: 1.3016\n",
      "Epoch [2/2], Loss: 1.4362\n",
      "Epoch [2/2], Loss: 1.5117\n",
      "Epoch [2/2], Loss: 1.4253\n",
      "Epoch [2/2], Loss: 1.3604\n",
      "Epoch [2/2], Loss: 1.3309\n",
      "Epoch [2/2], Loss: 1.3218\n",
      "Epoch [2/2], Loss: 1.6105\n",
      "Epoch [2/2], Loss: 0.9497\n",
      "Epoch [2/2], Loss: 1.2433\n",
      "Epoch [2/2], Loss: 1.2889\n",
      "Epoch [2/2], Loss: 1.0402\n",
      "Epoch [2/2], Loss: 1.5567\n",
      "Epoch [2/2], Loss: 1.2180\n",
      "Epoch [2/2], Loss: 1.3807\n",
      "Epoch [2/2], Loss: 1.5245\n",
      "Epoch [2/2], Loss: 1.1469\n",
      "Epoch [2/2], Loss: 1.0585\n",
      "Epoch [2/2], Loss: 1.1054\n",
      "Epoch [2/2], Loss: 1.3855\n",
      "Epoch [2/2], Loss: 1.3611\n",
      "Epoch [2/2], Loss: 1.5683\n",
      "Epoch [2/2], Loss: 1.4501\n",
      "Epoch [2/2], Loss: 1.3144\n",
      "Epoch [2/2], Loss: 1.3766\n",
      "Epoch [2/2], Loss: 1.4886\n",
      "Epoch [2/2], Loss: 1.4282\n",
      "Epoch [2/2], Loss: 1.2764\n",
      "Epoch [2/2], Loss: 1.6503\n",
      "Epoch [2/2], Loss: 1.4977\n",
      "Epoch [2/2], Loss: 1.4751\n",
      "Epoch [2/2], Loss: 1.2613\n",
      "Epoch [2/2], Loss: 1.3717\n",
      "Epoch [2/2], Loss: 1.3622\n",
      "Epoch [2/2], Loss: 1.2116\n",
      "Epoch [2/2], Loss: 1.4063\n",
      "Epoch [2/2], Loss: 1.2329\n",
      "Epoch [2/2], Loss: 1.5358\n",
      "Epoch [2/2], Loss: 1.3280\n",
      "Epoch [2/2], Loss: 1.3741\n",
      "Epoch [2/2], Loss: 1.4428\n",
      "Epoch [2/2], Loss: 1.1120\n",
      "Epoch [2/2], Loss: 1.2916\n",
      "Epoch [2/2], Loss: 1.4689\n",
      "Epoch [2/2], Loss: 1.3857\n",
      "Epoch [2/2], Loss: 1.2406\n",
      "Epoch [2/2], Loss: 1.1905\n",
      "Epoch [2/2], Loss: 1.2921\n",
      "Epoch [2/2], Loss: 1.3806\n",
      "Epoch [2/2], Loss: 1.2860\n",
      "Epoch [2/2], Loss: 1.2955\n",
      "Epoch [2/2], Loss: 1.0872\n",
      "Epoch [2/2], Loss: 0.9116\n",
      "Epoch [2/2], Loss: 1.2935\n",
      "Epoch [2/2], Loss: 1.2300\n",
      "Epoch [2/2], Loss: 1.4453\n",
      "Epoch [2/2], Loss: 1.3036\n",
      "Epoch [2/2], Loss: 1.1950\n",
      "Epoch [2/2], Loss: 1.4920\n",
      "Epoch [2/2], Loss: 1.3379\n",
      "Epoch [2/2], Loss: 0.9867\n",
      "Epoch [2/2], Loss: 1.2421\n",
      "Epoch [2/2], Loss: 1.3329\n",
      "Epoch [2/2], Loss: 1.3361\n",
      "Epoch [2/2], Loss: 1.2191\n",
      "Epoch [2/2], Loss: 1.0582\n",
      "Epoch [2/2], Loss: 1.1751\n",
      "Epoch [2/2], Loss: 1.3465\n",
      "Epoch [2/2], Loss: 1.2626\n",
      "Epoch [2/2], Loss: 1.3900\n",
      "Epoch [2/2], Loss: 1.6981\n",
      "Epoch [2/2], Loss: 1.4078\n",
      "Epoch [2/2], Loss: 1.4573\n",
      "Epoch [2/2], Loss: 1.3964\n",
      "Epoch [2/2], Loss: 1.1564\n",
      "Epoch [2/2], Loss: 1.3591\n",
      "Epoch [2/2], Loss: 1.2694\n",
      "Epoch [2/2], Loss: 1.5189\n",
      "Epoch [2/2], Loss: 1.4370\n",
      "Epoch [2/2], Loss: 1.3242\n",
      "Epoch [2/2], Loss: 1.4573\n",
      "Epoch [2/2], Loss: 1.2893\n",
      "Epoch [2/2], Loss: 1.2222\n",
      "Epoch [2/2], Loss: 1.1671\n",
      "Epoch [2/2], Loss: 1.4308\n",
      "Epoch [2/2], Loss: 1.2439\n",
      "Epoch [2/2], Loss: 1.1831\n",
      "Epoch [2/2], Loss: 1.1980\n",
      "Epoch [2/2], Loss: 0.9371\n",
      "Epoch [2/2], Loss: 1.2610\n",
      "Epoch [2/2], Loss: 1.1935\n",
      "Epoch [2/2], Loss: 1.0298\n",
      "Epoch [2/2], Loss: 1.1234\n",
      "Epoch [2/2], Loss: 1.2565\n",
      "Epoch [2/2], Loss: 1.4285\n",
      "Epoch [2/2], Loss: 1.0224\n",
      "Epoch [2/2], Loss: 1.2627\n",
      "Epoch [2/2], Loss: 1.3774\n",
      "Epoch [2/2], Loss: 1.2321\n",
      "Epoch [2/2], Loss: 1.4291\n",
      "Epoch [2/2], Loss: 0.9686\n",
      "Epoch [2/2], Loss: 1.4187\n",
      "Epoch [2/2], Loss: 1.1329\n",
      "Epoch [2/2], Loss: 1.1983\n",
      "Epoch [2/2], Loss: 1.2174\n",
      "Epoch [2/2], Loss: 1.1042\n",
      "Epoch [2/2], Loss: 1.0237\n",
      "Epoch [2/2], Loss: 1.2781\n",
      "Epoch [2/2], Loss: 1.1956\n",
      "Epoch [2/2], Loss: 1.0995\n",
      "Epoch [2/2], Loss: 1.0043\n",
      "Epoch [2/2], Loss: 1.3423\n",
      "Epoch [2/2], Loss: 1.3131\n",
      "Epoch [2/2], Loss: 1.2625\n",
      "Epoch [2/2], Loss: 1.0997\n",
      "Epoch [2/2], Loss: 1.2553\n",
      "Epoch [2/2], Loss: 1.1930\n",
      "Epoch [2/2], Loss: 1.0142\n",
      "Epoch [2/2], Loss: 1.0480\n",
      "Epoch [2/2], Loss: 1.4605\n",
      "Epoch [2/2], Loss: 0.9866\n",
      "Epoch [2/2], Loss: 0.9110\n",
      "Epoch [2/2], Loss: 1.3134\n",
      "Epoch [2/2], Loss: 1.0303\n",
      "Epoch [2/2], Loss: 1.2215\n",
      "Epoch [2/2], Loss: 1.0857\n",
      "Epoch [2/2], Loss: 0.9947\n",
      "Epoch [2/2], Loss: 1.2973\n",
      "Epoch [2/2], Loss: 1.2627\n",
      "Epoch [2/2], Loss: 1.1696\n",
      "Epoch [2/2], Loss: 1.4138\n",
      "Epoch [2/2], Loss: 1.3764\n",
      "Epoch [2/2], Loss: 1.2837\n",
      "Epoch [2/2], Loss: 1.0282\n",
      "Epoch [2/2], Loss: 1.5430\n",
      "Epoch [2/2], Loss: 1.2175\n",
      "Epoch [2/2], Loss: 1.0668\n",
      "Epoch [2/2], Loss: 1.0843\n",
      "Epoch [2/2], Loss: 0.9550\n",
      "Epoch [2/2], Loss: 1.0615\n",
      "Epoch [2/2], Loss: 1.3131\n",
      "Epoch [2/2], Loss: 1.3669\n",
      "Epoch [2/2], Loss: 1.3532\n",
      "Epoch [2/2], Loss: 1.0252\n",
      "Epoch [2/2], Loss: 1.0132\n",
      "Epoch [2/2], Loss: 0.9710\n",
      "Epoch [2/2], Loss: 1.1425\n",
      "Epoch [2/2], Loss: 1.0519\n",
      "Epoch [2/2], Loss: 0.9831\n",
      "Epoch [2/2], Loss: 1.0583\n",
      "Epoch [2/2], Loss: 1.0125\n",
      "Epoch [2/2], Loss: 1.0941\n",
      "Epoch [2/2], Loss: 1.1173\n",
      "Epoch [2/2], Loss: 1.0372\n",
      "Epoch [2/2], Loss: 1.2564\n",
      "Epoch [2/2], Loss: 0.8485\n",
      "Epoch [2/2], Loss: 1.0167\n",
      "Epoch [2/2], Loss: 1.0870\n",
      "Epoch [2/2], Loss: 0.9693\n",
      "Epoch [2/2], Loss: 1.1460\n",
      "Epoch [2/2], Loss: 1.0547\n",
      "Epoch [2/2], Loss: 0.8888\n",
      "Epoch [2/2], Loss: 1.0755\n",
      "Epoch [2/2], Loss: 0.9201\n",
      "Epoch [2/2], Loss: 0.9980\n",
      "Epoch [2/2], Loss: 1.1986\n",
      "Epoch [2/2], Loss: 1.1206\n",
      "Epoch [2/2], Loss: 1.0436\n",
      "Epoch [2/2], Loss: 1.2296\n",
      "Epoch [2/2], Loss: 0.9369\n",
      "Epoch [2/2], Loss: 1.3104\n",
      "Epoch [2/2], Loss: 1.1280\n",
      "Epoch [2/2], Loss: 0.8066\n",
      "Epoch [2/2], Loss: 1.1798\n",
      "Epoch [2/2], Loss: 1.4914\n",
      "Epoch [2/2], Loss: 0.9148\n",
      "Epoch [2/2], Loss: 1.0363\n",
      "Epoch [2/2], Loss: 1.1691\n",
      "Epoch [2/2], Loss: 1.1986\n",
      "Epoch [2/2], Loss: 1.0486\n",
      "Epoch [2/2], Loss: 1.1890\n",
      "Epoch [2/2], Loss: 1.0821\n",
      "Epoch [2/2], Loss: 1.0873\n",
      "Epoch [2/2], Loss: 1.2419\n",
      "Epoch [2/2], Loss: 1.2235\n",
      "Epoch [2/2], Loss: 1.3300\n",
      "Epoch [2/2], Loss: 1.3907\n",
      "Epoch [2/2], Loss: 1.1887\n",
      "Epoch [2/2], Loss: 1.7184\n",
      "Epoch [2/2], Loss: 1.1304\n",
      "Epoch [2/2], Loss: 1.1778\n",
      "Epoch [2/2], Loss: 1.1515\n",
      "Epoch [2/2], Loss: 1.0827\n",
      "Epoch [2/2], Loss: 0.8565\n",
      "Epoch [2/2], Loss: 1.0814\n",
      "Epoch [2/2], Loss: 1.3093\n",
      "Epoch [2/2], Loss: 1.0698\n",
      "Epoch [2/2], Loss: 0.9018\n",
      "Epoch [2/2], Loss: 0.9893\n",
      "Epoch [2/2], Loss: 1.3020\n",
      "Epoch [2/2], Loss: 1.5431\n",
      "Epoch [2/2], Loss: 1.3568\n",
      "Epoch [2/2], Loss: 0.9256\n",
      "Epoch [2/2], Loss: 1.0273\n",
      "Epoch [2/2], Loss: 1.1063\n",
      "Epoch [2/2], Loss: 1.1720\n",
      "Epoch [2/2], Loss: 0.9992\n",
      "Epoch [2/2], Loss: 1.3602\n",
      "Epoch [2/2], Loss: 1.1554\n",
      "Epoch [2/2], Loss: 1.0868\n",
      "Epoch [2/2], Loss: 1.3872\n",
      "Epoch [2/2], Loss: 1.3273\n",
      "Epoch [2/2], Loss: 1.0765\n",
      "Epoch [2/2], Loss: 1.4089\n",
      "Epoch [2/2], Loss: 1.0097\n",
      "Epoch [2/2], Loss: 1.1414\n",
      "Epoch [2/2], Loss: 1.1182\n",
      "Epoch [2/2], Loss: 1.0807\n",
      "Epoch [2/2], Loss: 1.0435\n",
      "Epoch [2/2], Loss: 1.0137\n",
      "Epoch [2/2], Loss: 1.0874\n",
      "Epoch [2/2], Loss: 1.1236\n",
      "Epoch [2/2], Loss: 0.9894\n",
      "Epoch [2/2], Loss: 1.1116\n",
      "Epoch [2/2], Loss: 1.5519\n",
      "Epoch [2/2], Loss: 1.1478\n",
      "Epoch [2/2], Loss: 1.1021\n",
      "Epoch [2/2], Loss: 0.9308\n",
      "Epoch [2/2], Loss: 1.3713\n",
      "Epoch [2/2], Loss: 1.0183\n",
      "Epoch [2/2], Loss: 0.9924\n",
      "Epoch [2/2], Loss: 1.3317\n",
      "Epoch [2/2], Loss: 1.1588\n",
      "Epoch [2/2], Loss: 1.1699\n",
      "Epoch [2/2], Loss: 0.9225\n",
      "Epoch [2/2], Loss: 1.1505\n",
      "Epoch [2/2], Loss: 0.9877\n",
      "Epoch [2/2], Loss: 1.2618\n",
      "Epoch [2/2], Loss: 0.8888\n",
      "Epoch [2/2], Loss: 1.0084\n",
      "Epoch [2/2], Loss: 1.0404\n",
      "Epoch [2/2], Loss: 1.0810\n",
      "Epoch [2/2], Loss: 1.0344\n",
      "Epoch [2/2], Loss: 1.0550\n",
      "Epoch [2/2], Loss: 0.8178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2], Loss: 1.0525\n",
      "Epoch [2/2], Loss: 1.0359\n",
      "Epoch [2/2], Loss: 0.9554\n",
      "Epoch [2/2], Loss: 0.8278\n",
      "Epoch [2/2], Loss: 1.1552\n",
      "Epoch [2/2], Loss: 1.0757\n",
      "Epoch [2/2], Loss: 0.9713\n",
      "Epoch [2/2], Loss: 0.9271\n",
      "Epoch [2/2], Loss: 1.1664\n",
      "Epoch [2/2], Loss: 1.1403\n",
      "Epoch [2/2], Loss: 1.1706\n",
      "Epoch [2/2], Loss: 1.0756\n",
      "Epoch [2/2], Loss: 0.8560\n",
      "Epoch [2/2], Loss: 0.8953\n",
      "Epoch [2/2], Loss: 0.9503\n",
      "Epoch [2/2], Loss: 1.0154\n",
      "Epoch [2/2], Loss: 1.0288\n",
      "Epoch [2/2], Loss: 1.0418\n",
      "Epoch [2/2], Loss: 1.3232\n",
      "Epoch [2/2], Loss: 1.0055\n",
      "Epoch [2/2], Loss: 1.0351\n",
      "Epoch [2/2], Loss: 1.1072\n",
      "Epoch [2/2], Loss: 0.9696\n",
      "Epoch [2/2], Loss: 0.9553\n",
      "Epoch [2/2], Loss: 0.9879\n",
      "Epoch [2/2], Loss: 1.3910\n",
      "Epoch [2/2], Loss: 0.9905\n",
      "Epoch [2/2], Loss: 0.9135\n",
      "Epoch [2/2], Loss: 1.1097\n",
      "Epoch [2/2], Loss: 1.0943\n",
      "Epoch [2/2], Loss: 0.8785\n",
      "Epoch [2/2], Loss: 0.9972\n",
      "Epoch [2/2], Loss: 1.0391\n",
      "Epoch [2/2], Loss: 1.0216\n",
      "Epoch [2/2], Loss: 1.1403\n",
      "Epoch [2/2], Loss: 1.0330\n",
      "Epoch [2/2], Loss: 0.9070\n",
      "Epoch [2/2], Loss: 0.7351\n",
      "Epoch [2/2], Loss: 1.1691\n",
      "Epoch [2/2], Loss: 0.8470\n",
      "Epoch [2/2], Loss: 1.1110\n",
      "Epoch [2/2], Loss: 1.2050\n",
      "Epoch [2/2], Loss: 0.9329\n",
      "Epoch [2/2], Loss: 0.9244\n",
      "Epoch [2/2], Loss: 0.8587\n",
      "Epoch [2/2], Loss: 0.7930\n",
      "Epoch [2/2], Loss: 1.0091\n",
      "Epoch [2/2], Loss: 0.9697\n",
      "Epoch [2/2], Loss: 0.9708\n",
      "Epoch [2/2], Loss: 1.0177\n",
      "Epoch [2/2], Loss: 0.8852\n",
      "Epoch [2/2], Loss: 1.0933\n",
      "Epoch [2/2], Loss: 0.9168\n",
      "Epoch [2/2], Loss: 0.9405\n",
      "Epoch [2/2], Loss: 0.9965\n",
      "Epoch [2/2], Loss: 1.0310\n",
      "Epoch [2/2], Loss: 0.8758\n",
      "Epoch [2/2], Loss: 0.6830\n",
      "Epoch [2/2], Loss: 0.7425\n",
      "Epoch [2/2], Loss: 0.8895\n",
      "Epoch [2/2], Loss: 0.9466\n",
      "Epoch [2/2], Loss: 1.0121\n",
      "Epoch [2/2], Loss: 0.7533\n",
      "Epoch [2/2], Loss: 0.7639\n",
      "Epoch [2/2], Loss: 0.7140\n",
      "Epoch [2/2], Loss: 1.1775\n",
      "Epoch [2/2], Loss: 0.8500\n",
      "Epoch [2/2], Loss: 0.8541\n",
      "Epoch [2/2], Loss: 1.1093\n",
      "Epoch [2/2], Loss: 1.1919\n",
      "Epoch [2/2], Loss: 0.9918\n",
      "Epoch [2/2], Loss: 0.9320\n",
      "Epoch [2/2], Loss: 0.8380\n",
      "Epoch [2/2], Loss: 1.0808\n",
      "Epoch [2/2], Loss: 0.8566\n",
      "Epoch [2/2], Loss: 0.8893\n",
      "Epoch [2/2], Loss: 1.1721\n",
      "Epoch [2/2], Loss: 0.9311\n",
      "Epoch [2/2], Loss: 0.9557\n",
      "Epoch [2/2], Loss: 1.4904\n",
      "Epoch [2/2], Loss: 0.7561\n",
      "Epoch [2/2], Loss: 0.7421\n",
      "Epoch [2/2], Loss: 1.1443\n",
      "Epoch [2/2], Loss: 1.0944\n",
      "Epoch [2/2], Loss: 1.0503\n",
      "Epoch [2/2], Loss: 0.8420\n",
      "Epoch [2/2], Loss: 0.9837\n",
      "Epoch [2/2], Loss: 0.8882\n",
      "Epoch [2/2], Loss: 0.7309\n",
      "Epoch [2/2], Loss: 0.8869\n",
      "Epoch [2/2], Loss: 0.7667\n",
      "Epoch [2/2], Loss: 1.0511\n",
      "Epoch [2/2], Loss: 1.1487\n",
      "Epoch [2/2], Loss: 0.9049\n",
      "Epoch [2/2], Loss: 0.9418\n",
      "Epoch [2/2], Loss: 0.8861\n",
      "Epoch [2/2], Loss: 0.8208\n",
      "Epoch [2/2], Loss: 1.0848\n",
      "Epoch [2/2], Loss: 1.1530\n",
      "Epoch [2/2], Loss: 0.8582\n",
      "Epoch [2/2], Loss: 0.8795\n",
      "Epoch [2/2], Loss: 1.1482\n",
      "Epoch [2/2], Loss: 0.9808\n",
      "Epoch [2/2], Loss: 0.7558\n",
      "Epoch [2/2], Loss: 0.8520\n",
      "Epoch [2/2], Loss: 0.7459\n",
      "Epoch [2/2], Loss: 0.8767\n",
      "Epoch [2/2], Loss: 0.7903\n",
      "Epoch [2/2], Loss: 0.9159\n",
      "Epoch [2/2], Loss: 0.9169\n",
      "Epoch [2/2], Loss: 0.8762\n",
      "Epoch [2/2], Loss: 0.8080\n",
      "Epoch [2/2], Loss: 0.4816\n",
      "Epoch [2/2], Loss: 0.9069\n",
      "Epoch [2/2], Loss: 0.8381\n",
      "Epoch [2/2], Loss: 0.6782\n",
      "Epoch [2/2], Loss: 0.7504\n",
      "Epoch [2/2], Loss: 0.8101\n",
      "Epoch [2/2], Loss: 0.5362\n",
      "Epoch [2/2], Loss: 0.9008\n",
      "Epoch [2/2], Loss: 0.6351\n",
      "Epoch [2/2], Loss: 0.8445\n",
      "Epoch [2/2], Loss: 0.8907\n",
      "Epoch [2/2], Loss: 0.6070\n",
      "Epoch [2/2], Loss: 0.7704\n",
      "Epoch [2/2], Loss: 0.9753\n",
      "Epoch [2/2], Loss: 0.7413\n",
      "Epoch [2/2], Loss: 0.6806\n",
      "Epoch [2/2], Loss: 1.0058\n",
      "Epoch [2/2], Loss: 0.7414\n",
      "Epoch [2/2], Loss: 1.2079\n",
      "Epoch [2/2], Loss: 0.8401\n",
      "Epoch [2/2], Loss: 0.7317\n",
      "Epoch [2/2], Loss: 0.7836\n",
      "Epoch [2/2], Loss: 0.8104\n",
      "Epoch [2/2], Loss: 0.7705\n",
      "Epoch [2/2], Loss: 0.8626\n",
      "Epoch [2/2], Loss: 0.7335\n",
      "Epoch [2/2], Loss: 0.9447\n",
      "Epoch [2/2], Loss: 0.9956\n",
      "Epoch [2/2], Loss: 0.8188\n",
      "Epoch [2/2], Loss: 0.5449\n",
      "Epoch [2/2], Loss: 1.2585\n",
      "Epoch [2/2], Loss: 1.0610\n",
      "Epoch [2/2], Loss: 0.7456\n",
      "Epoch [2/2], Loss: 1.1209\n",
      "Epoch [2/2], Loss: 0.8034\n",
      "Epoch [2/2], Loss: 0.9940\n",
      "Epoch [2/2], Loss: 0.5209\n",
      "Epoch [2/2], Loss: 0.8161\n",
      "Epoch [2/2], Loss: 1.0123\n",
      "Epoch [2/2], Loss: 0.7962\n",
      "Epoch [2/2], Loss: 0.8622\n",
      "Epoch [2/2], Loss: 1.0013\n",
      "Epoch [2/2], Loss: 0.8351\n",
      "Epoch [2/2], Loss: 0.8734\n",
      "Epoch [2/2], Loss: 0.6486\n",
      "Epoch [2/2], Loss: 0.6728\n",
      "Epoch [2/2], Loss: 0.8529\n",
      "Epoch [2/2], Loss: 0.7582\n",
      "Epoch [2/2], Loss: 0.7495\n",
      "Epoch [2/2], Loss: 0.9483\n",
      "Epoch [2/2], Loss: 0.9003\n",
      "Epoch [2/2], Loss: 0.6986\n",
      "Epoch [2/2], Loss: 0.7187\n",
      "Epoch [2/2], Loss: 0.7428\n",
      "Epoch [2/2], Loss: 0.9524\n",
      "Epoch [2/2], Loss: 0.7791\n",
      "Epoch [2/2], Loss: 1.1075\n",
      "Epoch [2/2], Loss: 0.6489\n",
      "Epoch [2/2], Loss: 0.6642\n",
      "Epoch [2/2], Loss: 0.6882\n",
      "Epoch [2/2], Loss: 0.9484\n",
      "Epoch [2/2], Loss: 0.9815\n",
      "Epoch [2/2], Loss: 1.0251\n",
      "Epoch [2/2], Loss: 0.7768\n",
      "Epoch [2/2], Loss: 0.8647\n",
      "Epoch [2/2], Loss: 0.7570\n",
      "Epoch [2/2], Loss: 0.9755\n",
      "Epoch [2/2], Loss: 0.6121\n",
      "Epoch [2/2], Loss: 0.8794\n",
      "Epoch [2/2], Loss: 0.9950\n",
      "Epoch [2/2], Loss: 1.1175\n",
      "Epoch [2/2], Loss: 1.1239\n",
      "Epoch [2/2], Loss: 0.6250\n",
      "Epoch [2/2], Loss: 0.8293\n",
      "Epoch [2/2], Loss: 1.0191\n",
      "Epoch [2/2], Loss: 0.7732\n",
      "Epoch [2/2], Loss: 0.7272\n",
      "Epoch [2/2], Loss: 0.8856\n",
      "Epoch [2/2], Loss: 0.9907\n",
      "Epoch [2/2], Loss: 0.8074\n",
      "Epoch [2/2], Loss: 0.8069\n",
      "Epoch [2/2], Loss: 0.9697\n",
      "Epoch [2/2], Loss: 0.7985\n",
      "Epoch [2/2], Loss: 0.6339\n",
      "Epoch [2/2], Loss: 1.1118\n",
      "Epoch [2/2], Loss: 0.8164\n",
      "Epoch [2/2], Loss: 0.5939\n",
      "Epoch [2/2], Loss: 0.8744\n",
      "Epoch [2/2], Loss: 1.1145\n",
      "Epoch [2/2], Loss: 0.6646\n",
      "Epoch [2/2], Loss: 0.7929\n",
      "Epoch [2/2], Loss: 0.7404\n",
      "Epoch [2/2], Loss: 0.6491\n",
      "Epoch [2/2], Loss: 0.7697\n",
      "Epoch [2/2], Loss: 0.6656\n",
      "Epoch [2/2], Loss: 0.6511\n",
      "Epoch [2/2], Loss: 0.8490\n",
      "Epoch [2/2], Loss: 0.9137\n",
      "Epoch [2/2], Loss: 0.6354\n",
      "Epoch [2/2], Loss: 0.7041\n",
      "Epoch [2/2], Loss: 0.6827\n",
      "Epoch [2/2], Loss: 1.0272\n",
      "Epoch [2/2], Loss: 0.7335\n",
      "Epoch [2/2], Loss: 1.0694\n",
      "Epoch [2/2], Loss: 0.9552\n",
      "Epoch [2/2], Loss: 0.7362\n",
      "Epoch [2/2], Loss: 0.8040\n",
      "Epoch [2/2], Loss: 0.8597\n",
      "Epoch [2/2], Loss: 0.6654\n",
      "Epoch [2/2], Loss: 0.7360\n",
      "Epoch [2/2], Loss: 0.7830\n",
      "Epoch [2/2], Loss: 0.7895\n",
      "Epoch [2/2], Loss: 0.6846\n",
      "Epoch [2/2], Loss: 0.6373\n",
      "Epoch [2/2], Loss: 0.6695\n",
      "Epoch [2/2], Loss: 0.5960\n",
      "Epoch [2/2], Loss: 0.8946\n",
      "Epoch [2/2], Loss: 0.9534\n",
      "Epoch [2/2], Loss: 0.6734\n",
      "Epoch [2/2], Loss: 0.7644\n",
      "Epoch [2/2], Loss: 0.6029\n",
      "Epoch [2/2], Loss: 0.9471\n",
      "Epoch [2/2], Loss: 0.5706\n",
      "Epoch [2/2], Loss: 0.6414\n",
      "Epoch [2/2], Loss: 0.9991\n",
      "Epoch [2/2], Loss: 0.8446\n",
      "Epoch [2/2], Loss: 0.8596\n",
      "Epoch [2/2], Loss: 0.8978\n",
      "Epoch [2/2], Loss: 0.7828\n",
      "Epoch [2/2], Loss: 0.7049\n",
      "Epoch [2/2], Loss: 0.7314\n",
      "Epoch [2/2], Loss: 0.6634\n",
      "Epoch [2/2], Loss: 0.9165\n",
      "Epoch [2/2], Loss: 0.8475\n",
      "Epoch [2/2], Loss: 0.9782\n",
      "Epoch [2/2], Loss: 0.6311\n",
      "Epoch [2/2], Loss: 0.6979\n",
      "Epoch [2/2], Loss: 0.7046\n",
      "Epoch [2/2], Loss: 1.0453\n",
      "Epoch [2/2], Loss: 0.5674\n",
      "Epoch [2/2], Loss: 0.4685\n",
      "Epoch [2/2], Loss: 0.9916\n",
      "Epoch [2/2], Loss: 0.8758\n",
      "Epoch [2/2], Loss: 0.6109\n",
      "Epoch [2/2], Loss: 0.7516\n",
      "Epoch [2/2], Loss: 0.6320\n",
      "Epoch [2/2], Loss: 0.6811\n",
      "Epoch [2/2], Loss: 0.8103\n",
      "Epoch [2/2], Loss: 0.8903\n",
      "Epoch [2/2], Loss: 0.4235\n",
      "Epoch [2/2], Loss: 0.7563\n",
      "Epoch [2/2], Loss: 0.9107\n",
      "Epoch [2/2], Loss: 0.7961\n",
      "Epoch [2/2], Loss: 0.7168\n",
      "Epoch [2/2], Loss: 0.9453\n",
      "Epoch [2/2], Loss: 0.5659\n",
      "Epoch [2/2], Loss: 0.6634\n",
      "Epoch [2/2], Loss: 0.7097\n",
      "Epoch [2/2], Loss: 0.6766\n",
      "Epoch [2/2], Loss: 0.5859\n",
      "Epoch [2/2], Loss: 0.8005\n",
      "Epoch [2/2], Loss: 0.8181\n",
      "Epoch [2/2], Loss: 0.9771\n",
      "Epoch [2/2], Loss: 0.9262\n",
      "Epoch [2/2], Loss: 0.5589\n",
      "Epoch [2/2], Loss: 0.5368\n",
      "Epoch [2/2], Loss: 1.0118\n",
      "Epoch [2/2], Loss: 0.5610\n",
      "Epoch [2/2], Loss: 0.7329\n",
      "Epoch [2/2], Loss: 0.6564\n",
      "Epoch [2/2], Loss: 0.6658\n",
      "Epoch [2/2], Loss: 0.7972\n",
      "Epoch [2/2], Loss: 0.8098\n",
      "Epoch [2/2], Loss: 0.3771\n",
      "Epoch [2/2], Loss: 0.5756\n",
      "Epoch [2/2], Loss: 0.7498\n",
      "Epoch [2/2], Loss: 0.6784\n",
      "Epoch [2/2], Loss: 1.0858\n",
      "Epoch [2/2], Loss: 0.6954\n",
      "Epoch [2/2], Loss: 1.0758\n",
      "Epoch [2/2], Loss: 0.6628\n",
      "Epoch [2/2], Loss: 0.7126\n",
      "Epoch [2/2], Loss: 0.8604\n",
      "Epoch [2/2], Loss: 0.8478\n",
      "Epoch [2/2], Loss: 0.5253\n",
      "Epoch [2/2], Loss: 0.6149\n",
      "Epoch [2/2], Loss: 0.6759\n",
      "Epoch [2/2], Loss: 0.5370\n",
      "Epoch [2/2], Loss: 0.6541\n",
      "Epoch [2/2], Loss: 0.6697\n",
      "Epoch [2/2], Loss: 0.8524\n",
      "Epoch [2/2], Loss: 0.6869\n",
      "Epoch [2/2], Loss: 0.4893\n",
      "Epoch [2/2], Loss: 0.7635\n",
      "Epoch [2/2], Loss: 0.8423\n",
      "Epoch [2/2], Loss: 0.8284\n",
      "Epoch [2/2], Loss: 0.6252\n",
      "Epoch [2/2], Loss: 0.7113\n",
      "Epoch [2/2], Loss: 0.6678\n",
      "Epoch [2/2], Loss: 0.5411\n",
      "Epoch [2/2], Loss: 0.6285\n",
      "Epoch [2/2], Loss: 0.6503\n",
      "Epoch [2/2], Loss: 0.5749\n",
      "Epoch [2/2], Loss: 0.7192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2], Loss: 0.7309\n",
      "Epoch [2/2], Loss: 0.6005\n",
      "Epoch [2/2], Loss: 0.6408\n",
      "Epoch [2/2], Loss: 0.9657\n",
      "Epoch [2/2], Loss: 0.4903\n",
      "Epoch [2/2], Loss: 0.8308\n",
      "Epoch [2/2], Loss: 0.7741\n",
      "Epoch [2/2], Loss: 0.9151\n",
      "Epoch [2/2], Loss: 0.7666\n",
      "Epoch [2/2], Loss: 0.5804\n",
      "Epoch [2/2], Loss: 0.6437\n",
      "Epoch [2/2], Loss: 0.7126\n",
      "Epoch [2/2], Loss: 0.7399\n",
      "Epoch [2/2], Loss: 0.5268\n",
      "Epoch [2/2], Loss: 0.6451\n",
      "Epoch [2/2], Loss: 0.7675\n",
      "Epoch [2/2], Loss: 0.4846\n",
      "Epoch [2/2], Loss: 0.3952\n",
      "Epoch [2/2], Loss: 0.6587\n",
      "Epoch [2/2], Loss: 0.6587\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        super(CNN,self).__init__()\n",
    "        # input channels is 3 because of RGB channels\n",
    "        #((w-f+2P)/s) + 1\n",
    "#         self.conv_l1 = nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = 3)\n",
    "#         self.conv_l2 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3)\n",
    "        self.conv_l1 = nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = 3)\n",
    "        self.conv_l2 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3)\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        \n",
    "#         self.conv_l3 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3)\n",
    "#         self.conv_l4 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3)\n",
    "        self.conv_l3 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3)\n",
    "        self.conv_l4 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3)\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        \n",
    "        self.conv_l5 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3)\n",
    "        self.conv_l6 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3)\n",
    "        self.max_pool3 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        \n",
    "#         self.conv_l5 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3)\n",
    "#         self.conv_l6 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3)\n",
    "#         self.max_pool3 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(50176,128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128,num_classes)\n",
    "        \n",
    "    def forward(self,x):\n",
    "#         print(x.size())\n",
    "        out = self.conv_l1(x)\n",
    "#         print(out.size())\n",
    "        out = self.conv_l2(out)\n",
    "#         print(out.size())\n",
    "        out = self.max_pool1(out)\n",
    "#         print(out.size())\n",
    "        out = self.conv_l3(out)\n",
    "#         print(out.size())\n",
    "        out = self.conv_l4(out)\n",
    "#         print(out.size())\n",
    "        out = self.max_pool2(out)\n",
    "    \n",
    "        out = self.conv_l5(out)\n",
    "#         print(out.size())\n",
    "        out = self.conv_l6(out)\n",
    "#         print(out.size())\n",
    "        out = self.max_pool2(out)\n",
    "#         print(out.size())\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        \n",
    "#         print(out.size())\n",
    "        out = self.fc1(out)\n",
    "#         print(out.size())\n",
    "        out = self.relu1(out)\n",
    "#         print(out.size())\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# layers = [module for module in model.modules()][1:]\n",
    "# summary(CNN(12),(32, 3, 256, 256))\n",
    "# layers\n",
    "\n",
    "model = CNN(num_classes)\n",
    "print(model)\n",
    "\n",
    "# Set Loss function with criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Set optimizer with optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n",
    "\n",
    "# total_step = len(train_loader)\n",
    "total_step = len(train_loader_balanced)\n",
    "\n",
    "best_score = None\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\t#Load in the data in batches using the train_loader object\n",
    "    for i, (images, labels) in enumerate(train_loader_balanced):\n",
    "#     for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "        \n",
    "    if best_score is None:\n",
    "        best_score = loss\n",
    "    else:\n",
    "            # Check if val_loss improves or not.\n",
    "        if loss < best_score:\n",
    "                # val_loss improves, we update the latest best_score, \n",
    "                # and save the current model\n",
    "            best_score = loss\n",
    "            torch.save(model.state_dict(), CNN_PATH)\n",
    "        else:\n",
    "                # val_loss does not improve, we increase the counter, \n",
    "                # stop training if it exceeds the amount of patience\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                break\n",
    "    if epoch == num_epochs:\n",
    "        torch.save(model.state_dict(), CNN_PATH)\n",
    "\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0465b3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv_l1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv_l2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv_l3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv_l4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv_l5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv_l6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (max_pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=50176, out_features=128, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7c1492f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 40000 train images: 80.9675 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader_balanced:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    \n",
    "    print('Accuracy of the network on the {} train images: {} %'.format(total, 100 * correct / total))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38c4a484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 45.4 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(total, 100 * correct / total))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "926f7ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 254, 254]             896\n",
      "            Conv2d-2         [-1, 32, 252, 252]           9,248\n",
      "         MaxPool2d-3         [-1, 32, 126, 126]               0\n",
      "            Conv2d-4         [-1, 64, 124, 124]          18,496\n",
      "            Conv2d-5         [-1, 64, 122, 122]          36,928\n",
      "         MaxPool2d-6           [-1, 64, 61, 61]               0\n",
      "            Conv2d-7           [-1, 64, 59, 59]          36,928\n",
      "            Conv2d-8           [-1, 64, 57, 57]          36,928\n",
      "         MaxPool2d-9           [-1, 64, 28, 28]               0\n",
      "           Linear-10                  [-1, 128]       6,422,656\n",
      "             ReLU-11                  [-1, 128]               0\n",
      "           Linear-12                   [-1, 12]           1,548\n",
      "================================================================\n",
      "Total params: 6,563,628\n",
      "Trainable params: 6,563,628\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 55.39\n",
      "Params size (MB): 25.04\n",
      "Estimated Total Size (MB): 81.18\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(CNN(12),(3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09a0fb21",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mtrain_loader\u001b[49m))\n\u001b[1;32m      2\u001b[0m images\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12cc871",
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4309e2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6b5fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.transforms.functional.get_image_size(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879c1695",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_df.str_label[1500])\n",
    "Image.open(test_df.img_path[1500])\n",
    "print(train_df.str_label[1000],train_df.img_path[1000])\n",
    "Image.open(train_df.img_path[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f5e9cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(train_df.str_label[1000],train_df.img_path[1000])\n",
    "Image.open(train_df.img_path[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d569818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose(\n",
    "#         [transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.5,0.5,0.5),(0.5, 0.5, 0.5))]\n",
    "#     )\n",
    "\n",
    "# test_data = datasets.ImageFolder('/workspace/resnet/data/raw/test/', transform=transform)\n",
    "# test_data_loader  = data.DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4) \n",
    "\n",
    "# train_data = datasets.ImageFolder('/workspace/resnet/data/raw/train/', transform=transform)\n",
    "# train_data_loader  = data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4) \n",
    "\n",
    "# train_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
